{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYwN/HA4Lp7AeyTsmn17gS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyxxnii/Tave-6th-Project/blob/master/NIPA%20%EB%B3%B8%EC%84%A0%20-%20%EC%9C%A1%EA%B5%B0%20%EB%AF%BC%EC%9B%90%20%EB%B6%84%EB%A5%98%20%EB%AA%A8%EB%8D%B8_try2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VImMyS7hPPi_"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_path = '../data/.train/.task145/data/train.tsv'\n",
        "test_path = '../data/.train/.task145/data/test.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL20opcbPR2-"
      },
      "source": [
        "train_df = pd.read_csv(train_path, sep='\\t')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psXFoPGpPR_U"
      },
      "source": [
        "def read_documents(filename):\n",
        "    with open(filename, encoding=\"utf-8\") as f: # 윈도우는 꼭 encoding 해줘야 함\n",
        "        documents = [line.split('\\t') for line in f.read().splitlines()] \n",
        "        documents = documents[1:] # 첫번째 줄이 카테고리 이름적혀있는 줄이라서 날려버림\n",
        "        \n",
        "    return documents\n",
        "\n",
        "test = read_documents(test_path)\n",
        "test_df = pd.DataFrame(test)\n",
        "test_df.columns = [\"comment\", \"tag\"]\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g5Qovl1PSCb"
      },
      "source": [
        "import konlpy \n",
        "from konlpy.tag import Mecab, Kkma, Okt, Komoran\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "def text_cleaning(doc):\n",
        "    # 한국어를 제외한 글자를 제거하는 함수.\n",
        "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
        "    return doc\n",
        "\n",
        "def define_stopwords(path):\n",
        "    SW = set()\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for word in f:\n",
        "            SW.add(word)\n",
        "            \n",
        "    return SW\n",
        "\n",
        "# okt.morphs(sentence)\n",
        "\n",
        "def text_tokenizing(doc):\n",
        "    return [word for word in mecab.morphs(doc) if word not in SW and len(word) > 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCqWFjFuPSlj"
      },
      "source": [
        "import konlpy \n",
        "from konlpy.tag import Mecab, Kkma, Okt, Komoran\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "# 형태소 분석기 불러오기\n",
        "okt = Okt() # 혹시나 mecab 설치 실패한 분들 위해서\n",
        "mecab = Mecab()\n",
        "\n",
        "SW = define_stopwords(\"./stopwords-ko.txt\")\n",
        "\n",
        "for i in range(len(train_df)):\n",
        "    text = text_cleaning(train_df['comment'][i])\n",
        "    train_df['comment'][i] = text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjKpzsVBPSps"
      },
      "source": [
        "## Deep Neural Network로 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWZGTavVPStL"
      },
      "source": [
        "# 필요한 라이브러리 불러오기\n",
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLCHp3EfPSwb"
      },
      "source": [
        "## Set Hyperparameter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBFxJwPAPSzM"
      },
      "source": [
        "max_words = 35000 # feature selection 방법\n",
        "max_len = 20 # 문서의 최대 길이\n",
        "batch_size = 16\n",
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0GVOXkDPS_N"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y_target = train_df['tag']\n",
        "X_features = train_df.drop('tag', axis=1, inplace=False)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, \n",
        "                                                    test_size=0.2,\n",
        "                                                   random_state=156)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFnNSx2RPTCk"
      },
      "source": [
        "train_words = [] \n",
        "for i in range(len(X_train)):\n",
        "    text = text_tokenizing(X_train['comment'].iloc[i])\n",
        "    train_words.append(text)\n",
        "    \n",
        "test_words = []\n",
        "for i in range(len(X_test)):\n",
        "    text = text_tokenizing(X_test['comment'].iloc[i])\n",
        "    test_words.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdgy0NN6PTFd"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_words) # tokenizer쓸 때 꼭 fit을 해줘야 해\n",
        "\n",
        "# LSTM의 input으로 넣기 위한 변환 작업\n",
        "tokenizer_train_words = tokenizer.texts_to_sequences(train_words)\n",
        "tokenizer_test_words = tokenizer.texts_to_sequences(test_words)\n",
        "\n",
        "# 크기를 맞춰주기 위한 zero padding\n",
        "X_train = pad_sequences(tokenizer_train_words, padding='pre', maxlen=max_len)\n",
        "X_test = pad_sequences(tokenizer_test_words, padding='pre', maxlen=max_len)\n",
        "# value=0 : padding할 때 0으로 채워라(default값이긴 하지만 좀 더 명시해주기위해)\n",
        "# padding='pre' : 앞부터 채워라\n",
        "\n",
        "# 학습 가능한 형태로 최종 변환.\n",
        "# # ds : data structure\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(batch_size) \n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
        "# 10000개씩 shuffle, test에서는 당연히 shuffle 필요 없겠지"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU7XyGlXPTIN"
      },
      "source": [
        "Y_TRAIN = []\n",
        "Y_TEST = []\n",
        "for i in range(len(y_train)):\n",
        "    if y_train.iloc[i] == 2:\n",
        "        Y_TRAIN.append([0, 0, 1])\n",
        "    elif y_train.iloc[i] == 1:\n",
        "        Y_TRAIN.append([0, 1, 0])\n",
        "    elif y_train.iloc[i] == 0:\n",
        "        Y_TRAIN.append([1, 0, 0])\n",
        "        \n",
        "for i in range(len(y_test)):\n",
        "    if y_test.iloc[0] == 2:\n",
        "        Y_TEST.append([0, 0, 1])    \n",
        "    elif y_test.iloc[0] == 1:\n",
        "        Y_TEST.append([0, 1, 0])\n",
        "    elif y_test.iloc[0] == 0: \n",
        "        Y_TEST.append([1, 0, 0])\n",
        "        \n",
        "Y_TRAIN = np.array(Y_TRAIN)\n",
        "Y_TEST = np.array(Y_TEST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JU7hzzYPsG4"
      },
      "source": [
        "## Set Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLVbU27xPTLS"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBecEMZ4Pt3U"
      },
      "source": [
        "model = Sequential() \n",
        "model.add(Embedding(max_words, 100)) \n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[recall, precision, f1score]) \n",
        "earlystopper = tf.keras.callbacks.EarlyStopping(monitor=\"val_f1score\", patience=10, verbose=1)\n",
        "history = model.fit(X_train, Y_TRAIN, epochs=EPOCHS, batch_size=batch_size, callbacks=[earlystopper])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKXgT3LjPTOG"
      },
      "source": [
        "# 실행, 결과 저장.\n",
        "\n",
        "predict = model.predict(X_test)\n",
        "predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h638TpuHPTRR"
      },
      "source": [
        "result = [np.argmax(value) for value in predict]\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs30YZskPTUD"
      },
      "source": [
        "loss_and_metrics = model.evaluate(X_test, Y_TEST, batch_size=16)\n",
        "print('## evaluation loss and_metrics ##')\n",
        "print(loss_and_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tde6TO5_PTWi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}