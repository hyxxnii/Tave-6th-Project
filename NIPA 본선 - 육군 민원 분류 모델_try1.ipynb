{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzLCh0EeIeUyGVqyhYOBLZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyxxnii/Tave-6th-Project/blob/master/NIPA%20%EB%B3%B8%EC%84%A0%20-%20%EC%9C%A1%EA%B5%B0%20%EB%AF%BC%EC%9B%90%20%EB%B6%84%EB%A5%98%20%EB%AA%A8%EB%8D%B8_try1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcy39gcggVTU"
      },
      "source": [
        "- 육군 SNS 계정 게시글 댓글 카테고리 분류(긍정 / 부정 / 중립)\n",
        "- 채점: f1-score(macro)\n",
        "-데이터 구조\n",
        "    - comment: 댓글내용\n",
        "    - label: comment 중립(0)/긍정(1)/부정(2)\n",
        "- 제출 양식: 컬럼 정보 없이 예측된 tag값만 존재하는 tsv파일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOs3B6NGg3Sq"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_path = '../data/.train/.task145ㅁ/data/train.tsv'\n",
        "test_path = '../data/.train/.task145/data/test.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwBw3EZUg31x"
      },
      "source": [
        "train_df = pd.read_csv(train_path, sep='\\t')\n",
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBnbU5BGg5Ru"
      },
      "source": [
        "def read_documents(filename):\n",
        "    with open(filename, encoding=\"utf-8\") as f: # 윈도우는 꼭 encoding 해줘야 함\n",
        "        documents = [line.split('\\t') for line in f.read().splitlines()] \n",
        "        documents = documents[1:] # 첫번째 줄이 카테고리 이름적혀있는 줄이라서 날려버림\n",
        "        \n",
        "    return documents\n",
        "\n",
        "test = read_documents(test_path)\n",
        "test_df = pd.DataFrame(test)\n",
        "test_df.columns = [\"comment\", \"tag\"]\n",
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMR_ITK0g7WN"
      },
      "source": [
        "test_df['tag'] = test_df['tag'].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyzQv_Gjg8tc"
      },
      "source": [
        "import konlpy \n",
        "from konlpy.tag import Mecab, Kkma, Okt, Komoran\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "def text_cleaning(doc):\n",
        "    # 한국어를 제외한 글자를 제거하는 함수.\n",
        "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
        "    return doc\n",
        "\n",
        "def define_stopwords(path):\n",
        "    SW = set()\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for word in f:\n",
        "            SW.add(word)\n",
        "            \n",
        "    return SW\n",
        "\n",
        "def text_tokenizing(doc):\n",
        "    return [word for word in mecab.nouns(doc) if word not in SW and len(word) > 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oin0IeLJg9Y0"
      },
      "source": [
        "#### 불러온 데이터를 품사 태그를 붙여서 토크나이징"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbQgS3lGg-9C"
      },
      "source": [
        "import konlpy \n",
        "from konlpy.tag import Mecab, Kkma, Okt, Komoran\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "# 형태소 분석기 불러오기\n",
        "okt = Okt()\n",
        "mecab = Mecab()\n",
        "\n",
        "SW = define_stopwords(\"./stopwords-ko.txt\")\n",
        "\n",
        "#  os.path.exists(): 어떤 파일이 디렉토리에 있는지 확인하는 library\n",
        "if os.path.exists(\"train_docs.json\"):\n",
        "    # train_docs.json:형태소 분석기 돌려서 나온 결과를 json형태로 저장한 파일\n",
        "    # 있으면 불러오기\n",
        "    with open(\"train_docs.json\", encoding='utf-8') as f:\n",
        "        train_data = json.load(f)\n",
        "    \n",
        "else:\n",
        "    # 없으면 형태소 분석해야지\n",
        "    # 리뷰 리스트, 레이블 가져오기\n",
        "    train_data = [(text_tokenizing(line[0]), line[1]) for line in train_df.values if text_tokenizing(line[0])]\n",
        "    \n",
        "    with open(\"train_docs.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(train_data, f, ensure_ascii=False, indent='\\t')\n",
        "        # ensure_ascii=False: 아스키코드 보존할거냐\n",
        "        # indent='\\t': 저장도 탭 기준으로\n",
        "        \n",
        "if os.path.exists(\"test_docs.json\"):\n",
        "    with open(\"test_docs.json\", encoding='utf-8') as f:\n",
        "        test_data = json.load(f)\n",
        "        \n",
        "else:\n",
        "    test_data = [(text_tokenizing(line[0]), line[1]) for line in test_df.values if text_tokenizing(line[0])]\n",
        "    \n",
        "    with open(\"test_docs.json\", \"w\", encoding='utf-8') as f:\n",
        "        json.dump(test_data, f, ensure_ascii=False, indent='\\t')\n",
        "        \n",
        "pprint(train_data[0])\n",
        "pprint(test_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itFo0wiXhAoF"
      },
      "source": [
        "print(len(train_df) - len(train_data))\n",
        "print(len(test_df) - len(test_data))\n",
        "# 정제하고 사라진 데이터 갯수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C62rweHhAy5"
      },
      "source": [
        "import nltk\n",
        "\n",
        "total_tokens = [token for doc in train_data for token in doc[0]]\n",
        "print(len(total_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4BrTXvbhA2R"
      },
      "source": [
        "text = nltk.Text(total_tokens, name='ARMY')\n",
        "print(len(set(text.tokens)))\n",
        "pprint(text.vocab().most_common(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8A3zNhRhA5E"
      },
      "source": [
        "# 여러 리스트들을 하나로 묶어 주는 함수입니다.\n",
        "def list_to_str(List): \n",
        "    return \" \".join(List)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3CZKIchA7n"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyEywNDFhA-o"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y_target = [label for _, label in train_data]\n",
        "X_features = [list_to_str(doc) for doc, _ in train_data]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, \n",
        "                                                    test_size=0.2,\n",
        "                                                   random_state=156)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t93HPymShIW1"
      },
      "source": [
        "# Linear classification에 필요한 라이브러리 불러오기\n",
        "from sklearn.pipeline import Pipeline # 편하게 learner들 만들 수 있다\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "\n",
        "# 학습 모델 파이프라인 만들기\n",
        "\n",
        "# 1. Perceptron linear classifier (W^Tx + b)\n",
        "learner = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=5)),\n",
        "    ('clf', SGDClassifier(loss='perceptron', penalty='l2',\n",
        "                         alpha=1e-4, random_state=42, \n",
        "                         max_iter=500)) # 성능을 높이고싶다면 loss='perceptron'이라면 penalty랑 alpha값 조정\n",
        "                        #max_iter은 올려도돼(수렴이 안됐다면 더 돌릴 수 있겠지)\n",
        "])\n",
        "\n",
        "# 총 4개의 SVC\n",
        "# 2. SVM with linear kernel\n",
        "learner2 = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=5)),\n",
        "    ('clf', SVC(kernel='linear')) \n",
        "    # 데이터에 noise가 껴있거나 데이터 분포가 선형으로 분류가 불가능한 것이더라도 kernel을 쓰면 선형이 되는 공간으로 옴(머신러닝에서 kernel이라는 공간변환기법쓰는 이유)\n",
        "    # 단점 : 어떤 kernel을 써야 성능이 가장 좋은지 알 수 없음(해봐야 암), 데이터 분포를 잘 안다면 사용할 수 있는데 대부분 고차원의 데이터 분포를 자세하게 알 수 있을리가 없지\n",
        "]) \n",
        "\n",
        "# 3. SVM with polynomial kernel\n",
        "learner3 = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=5)),\n",
        "    ('clf', SVC(kernel='poly', degree=8))\n",
        "]) # 8차원 준 이유는 이 degree도 정해진 게 없어서 높이면 높일 수록 오래 걸리는 대신에 좀 더 많이 구분함\n",
        "\n",
        "# 4. SVM with Radius Basis Function kernel\n",
        "learner4 = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=5)),\n",
        "    ('clf', SVC(kernel='rbf'))\n",
        "])\n",
        "\n",
        "# 5. SVM with sigmoid kernel\n",
        "learner5 = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=5)),\n",
        "    ('clf', SVC(kernel='sigmoid'))\n",
        "])\n",
        "\n",
        "# 6. Naive Bayes Classifier (Multinomial Ver.)\n",
        "learner6 = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=5)),\n",
        "    ('clf', MultinomialNB())\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCFJ3nZ3hBGw"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# X_train, X_test, y_train, y_test \n",
        "# 학습기 정하기\n",
        "classifier = learner\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "train_pred = classifier.predict(X_test)\n",
        "train_accuracy = accuracy_score(y_test, train_pred)\n",
        "train_f1score = f1_score(y_test, train_pred , average=\"macro\")\n",
        "train_precision = precision_score(y_test, train_pred, average=\"macro\")\n",
        "train_recall = recall_score(y_test, train_pred, average=\"macro\")\n",
        "\n",
        "print(\"Training Accuracy : %.3f\" % train_accuracy)\n",
        "print(\"Training F1_score : %.3f\" % train_f1score)\n",
        "print(\"Training precision : %.3f\" % train_precision)\n",
        "print(\"Training recall : %.3f\" % train_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLRbf7uhhBJy"
      },
      "source": [
        "classifier = learner2\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "train_pred = classifier.predict(X_test)\n",
        "train_accuracy = accuracy_score(y_test, train_pred)\n",
        "train_f1score = f1_score(y_test, train_pred , average=\"macro\")\n",
        "\n",
        "print(\"Training Accuracy : %.3f\" % train_accuracy)\n",
        "print(\"Training F1_score : %.3f\" % train_f1score)\n",
        "print(\"Training precision : %.3f\" % train_precision)\n",
        "print(\"Training recall : %.3f\" % train_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we3ys-yChBU5"
      },
      "source": [
        "classifier = learner6\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "train_pred = classifier.predict(X_test)\n",
        "train_accuracy = accuracy_score(y_test, train_pred)\n",
        "train_f1score = f1_score(y_test, train_pred , average=\"macro\")\n",
        "\n",
        "print(\"Training Accuracy : %.3f\" % train_accuracy)\n",
        "print(\"Training F1_score : %.3f\" % train_f1score)\n",
        "print(\"Training precision : %.3f\" % train_precision)\n",
        "print(\"Training recall : %.3f\" % train_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C91KTnsahOH0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBKyVc-FhONm"
      },
      "source": [
        "### 방법 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY-LZr9QhPBg"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# LogisticRegreesion의 C는 10으로 설정\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', CountVectorizer(min_df=5)),\n",
        "    ('lr_clf', LogisticRegression(C=10))])\n",
        "\n",
        "# Pipeline 객체를 이용해 fit(), predict()로 학습/예측 수행. predict_proba()는 roc_auc때문에 수행\n",
        "pipeline.fit(X_train, y_train)\n",
        "pred = pipeline.predict(X_test)\n",
        "pred_probs = pipeline.predict_proba(X_test)[:,1]\n",
        "\n",
        "train_accuracy = accuracy_score(y_test, pred)\n",
        "train_f1score = f1_score(y_test, pred , average=\"macro\")\n",
        "\n",
        "print(\"Training Accuracy : %.3f\" % train_accuracy)\n",
        "print(\"Training F1_score : %.3f\" % train_f1score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttBpfTK0hP0q"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(ngram_range=(1,2))),\n",
        "    ('lr_clf', LogisticRegression(C=10))])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "pred = pipeline.predict(X_test)\n",
        "pred_probs = pipeline.predict_proba(X_test)[:,1]\n",
        "\n",
        "train_accuracy = accuracy_score(y_test, pred)\n",
        "train_f1score = f1_score(y_test, pred , average=\"macro\")\n",
        "\n",
        "print(\"Training Accuracy : %.3f\" % train_accuracy)\n",
        "print(\"Training F1_score : %.3f\" % train_f1score)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}