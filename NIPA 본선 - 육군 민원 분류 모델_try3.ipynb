{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NIPA 본선 - 육군 민원 분류 모델_try3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYcFW5Of3VL9DVqqT9cIwj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyxxnii/Tave-6th-Project/blob/master/NIPA%20%EB%B3%B8%EC%84%A0%20-%20%EC%9C%A1%EA%B5%B0%20%EB%AF%BC%EC%9B%90%20%EB%B6%84%EB%A5%98%20%EB%AA%A8%EB%8D%B8_try3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWwvF31iGfEy"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_path = '../data/.train/.task145/data/train.tsv'\n",
        "test_path = '../data/.train/.task145/data/test.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llp7gYe4Gh81"
      },
      "source": [
        "train_df = pd.read_csv(train_path, sep='\\t')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppe_6VhcGiDW"
      },
      "source": [
        "def read_documents(filename):\n",
        "    with open(filename, encoding=\"utf-8\") as f: # 윈도우는 꼭 encoding 해줘야 함\n",
        "        documents = [line.split('\\t') for line in f.read().splitlines()] \n",
        "        documents = documents[1:] # 첫번째 줄이 카테고리 이름적혀있는 줄이라서 날려버림\n",
        "        \n",
        "    return documents\n",
        "\n",
        "test = read_documents(test_path)\n",
        "test_df = pd.DataFrame(test)\n",
        "test_df.columns = [\"comment\", \"tag\"]\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m65Jo3gTGiGH"
      },
      "source": [
        "train_org = train_df.copy()\n",
        "test_org = test_df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWvUkg_XGiI3"
      },
      "source": [
        "train_df['tag'].value_counts()\n",
        "# 불균형"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXbop9qWGiL7"
      },
      "source": [
        "import konlpy \n",
        "from konlpy.tag import Mecab, Kkma, Okt, Komoran\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "def text_cleaning(doc):\n",
        "    # 한국어를 제외한 글자를 제거하는 함수.\n",
        "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
        "    return doc\n",
        "\n",
        "def define_stopwords(path):\n",
        "    SW = set()\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for word in f:\n",
        "            SW.add(word)\n",
        "            \n",
        "    return SW\n",
        "\n",
        "def text_tokenizing(doc):\n",
        "    return [word for word in mecab.morphs(doc) if word not in SW and len(word) > 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dufmdGnGiPK"
      },
      "source": [
        "import konlpy \n",
        "from konlpy.tag import Mecab, Kkma, Okt, Komoran\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "# 형태소 분석기 불러오기\n",
        "okt = Okt() # 혹시나 mecab 설치 실패한 분들 위해서\n",
        "mecab = Mecab()\n",
        "\n",
        "SW = define_stopwords(\"./stopwords-ko.txt\")\n",
        "\n",
        "for i in range(len(train_df)):\n",
        "    text = text_cleaning(train_df['comment'][i])\n",
        "    train_df['comment'][i] = text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwODFxH9GiSG"
      },
      "source": [
        "###  BoW(Bag of Words) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z4L_TcYGiU-"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y_target = train_df['tag']\n",
        "X_features = train_df.drop('tag', axis=1, inplace=False)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, \n",
        "                                                    test_size=0.2,\n",
        "                                                   random_state=156)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCPJ56mEHTCu"
      },
      "source": [
        "### LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxggu6l0GiX9"
      },
      "source": [
        "\"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', LogisticRegression())])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2poYtSYGibC"
      },
      "source": [
        "# text cleaning X\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', LogisticRegression())])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upT6BGEqGid3"
      },
      "source": [
        "\n",
        "### MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s0VOm66Gigv"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', MultinomialNB())])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVaKTs28Gij_"
      },
      "source": [
        "# text Cleaning X\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', MultinomialNB())])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmIdnrTwGiqP"
      },
      "source": [
        "\n",
        "### SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yv7qnpZGiv5"
      },
      "source": [
        "# 성능 젤 good\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', SVC(kernel='linear'))])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poxFg49oGizW"
      },
      "source": [
        "# text Cleaning X\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', SVC(kernel='linear'))])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFGwWXM5Gi8X"
      },
      "source": [
        "### SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DxAiADHGi_k"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', SVC(kernel='rbf'))])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sg4ShfWGjE7"
      },
      "source": [
        "# text Cleaning X\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', SVC(kernel='rbf'))])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvvdl8bBGxKP"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', SVC(kernel='linear'))])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AImplebGi2k"
      },
      "source": [
        "### SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8l3AqXrGw7p"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', SGDClassifier(loss='perceptron', penalty='l2',\n",
        "                         alpha=1e-4, random_state=42, \n",
        "                         max_iter=500))])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDwZIItoGxAp"
      },
      "source": [
        "# text Cleaning X\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', SGDClassifier(loss='perceptron', penalty='l2',\n",
        "                         alpha=1e-4, random_state=42, \n",
        "                         max_iter=500))])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sArxNKuyGxDw"
      },
      "source": [
        "### LGBMClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxvEQigAGxGm"
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cnt_vect', TfidfVectorizer(tokenizer=text_tokenizing, ngram_range=(1,2), max_df=0.95)),\n",
        "    ('lr_clf', LGBMClassifier(n_estimators=500,\n",
        "                     num_leaves=24,\n",
        "                     boost_from_average=False,\n",
        "                    is_unbalance = True))])\n",
        "\n",
        "pipeline.fit(X_train['comment'], y_train)\n",
        "pred = pipeline.predict(X_test['comment'])\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJHPeLisGxNb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC1YirvRGxQ6"
      },
      "source": [
        "## SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g1HZJ_aGxXh"
      },
      "source": [
        "# SMOTE로 불균형 데이터 처리 후\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "smote = SMOTE(random_state = 150)\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=text_tokenizing, \n",
        "                             ngram_range=(1,2), \n",
        "                             max_df=0.95)\n",
        "\n",
        "tfidf_vect.fit(X_train['comment'])\n",
        "tfidf_vect_train = tfidf_vect.transform(X_train['comment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljwlt4xsGxgz"
      },
      "source": [
        "X_train_over, y_train_over = smote.fit_sample(tfidf_vect_train, y_train)\n",
        "\n",
        "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)\n",
        "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)\n",
        "print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y_train_over).value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHIIDHDaGxk4"
      },
      "source": [
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train_over, y_train_over)\n",
        "\n",
        "tfidf_vect_test = tfidf_vect.transform(X_test['comment'])\n",
        "pred_over = clf.predict(tfidf_vect_test)\n",
        "\n",
        "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred_over, average=\"macro\"),\n",
        "                                                precision_score(y_test, pred_over, average=\"macro\"), recall_score(y_test, pred_over, average=\"macro\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeCGY8RBGxao"
      },
      "source": [
        "#### Test 데이터 & Submit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwEfOZW6G-RZ"
      },
      "source": [
        "for i in range(len(test_df)):\n",
        "    text = text_cleaning(test_df['comment'][i])\n",
        "    test_df['comment'][i] = text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch7cNlwiG-98"
      },
      "source": [
        "# train데이터 적용했던 TfidfVectorizer이용하여 test데이터도 피처변환\n",
        "tfidf_matrix_test_df = tfidf_vect.transform(test_df['comment'])\n",
        "\n",
        "# classifier 는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n",
        "best_estimator_df = grid_cv.best_estimator_\n",
        "preds_test = best_estimator_df.predict(tfidf_matrix_test_df)\n",
        "\n",
        "#print('Logistic Regression 정확도: ', accuracy_score(y_test, preds))\n",
        "preds_test[:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orZsgFBKG_gz"
      },
      "source": [
        "# 성능 젤 good\n",
        "\n",
        "test_pred = pipeline.predict(test_df['comment'])\n",
        "test_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmuULy-wHAV9"
      },
      "source": [
        "type(test_pred)\n",
        "test_df['tag']= test_pred\n",
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6M0JNt0HBSm"
      },
      "source": [
        "submission = test_df['tag']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leSR6VIxHH4C"
      },
      "source": [
        "submission.to_csv('./submit_yh2_army.tsv',index=False, header=False, sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sxQqx4uHH65"
      },
      "source": [
        "from nipa.taskSubmit import nipa_submit\n",
        "\n",
        "team_id=\"1345\"\n",
        "task_no=\"145\"\n",
        "nipa_submit(team_id=team_id,\n",
        "            task_no=task_no,\n",
        "            result='./submit_yh2_army.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbekYivTHKBK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnBWOvbvHKDb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbsDSitdHKGE"
      },
      "source": [
        "### Word2Vec(Word Embedding to Vector)\n",
        "- BoW기법 -> 고차원의 sparse한 벡터 -> 성능 잘 안나옴\n",
        "- Word2Vec -> 저차원의 dense vector\n",
        "    - '주위 단어가 비슷하면 해당 단어의 의미는 유사하다'라는 아이디어에서 시작\n",
        "    - 거리가 가까울수록 의미가 비슷"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brqo90U8HNeg"
      },
      "source": [
        "train_df = train_org.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me7IeTtzHKI-"
      },
      "source": [
        "for i in range(len(train_df)):\n",
        "    text = text_cleaning(train_df['comment'][i])\n",
        "    train_df['comment'][i] = text\n",
        "\n",
        "for i in range(len(train_df)):\n",
        "    text = text_tokenizing(train_df['comment'][i])\n",
        "    train_df['comment'][i] = text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAMrVjAxHKLg"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y_target = train_df['tag']\n",
        "X_features = train_df.drop('tag', axis=1, inplace=False)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, \n",
        "                                                    test_size=0.2,\n",
        "                                                   random_state=156)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXudxlF8HN4A"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "embedding_model = Word2Vec(X_train['comment'], \n",
        "                           size=100, window=5, \n",
        "                           min_count=5, \n",
        "                           workers=4, \n",
        "                           iter=100, \n",
        "                           sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S28Q_qW2HOBQ"
      },
      "source": [
        "# check embedding result\n",
        "# 비교할 단어와 가장 비슷한(코사인 유사도가 큰) 100개 단어\n",
        "print(embedding_model.most_similar(positive=[\"사람\"], topn=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE6z9s4yHODs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}